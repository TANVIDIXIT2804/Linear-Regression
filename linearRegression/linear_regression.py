# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""

from jax import random, jit, vmap, grad, jacfwd, jacrev, hessian, value_and_grad
import matplotlib.pyplot as plt
from IPython.display import Image
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression as linreg
import jax
import jax.numpy as jnp
from matplotlib.ticker import LinearLocator
from matplotlib.animation import FuncAnimation
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D, art3d
from mpl_toolkits import mplot3d
from matplotlib import cm
from matplotlib.patches import Circle
import os
import imageio.v2 as imageio

np.random.seed(45)


class LinearRegression:
    def __init__(self, fit_intercept):
        # Initialize relevant variables
        """
        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).
        """
        self.fit_intercept = fit_intercept
        self.coef_ = None  # Replace with numpy array or pandas series of coefficients learned using using the fit methods
        self.all_coef = pd.DataFrame([])  # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)

    def add_one(self, X):
        if self.fit_intercept:
            return (np.asarray(pd.concat([pd.Series(np.ones(X.shape[0])), pd.DataFrame(X)], axis=1))
                if type(X) == np.ndarray or list
                else np.asarray(pd.concat([pd.Series(np.ones(X.shape[0])), X], axis=1)))
        else:
            return np.asarray(X)

    def fit_sklearn_LR(self, X, y):
        # Solve the linear regression problem by calling Linear Regression
        # from sklearn, with the relevant parameters
        lin = linreg(fit_intercept=self.fit_intercept)
        lin.fit(X, y)
        self.coef_ = np.zeros(X.shape[1] + 1)
        for i in range(len(self.coef_)):
            if i == 0:
                self.coef_[i] = lin.intercept_
            else:
                self.coef_[i] = lin.coef_[i - 1]

    def fit_normal_equations(self, X, y):
        # Solve the linear regression problem using the closed form solution
        # to the normal equation for minimizing ||Wx - y||_2^2
        X_temp = self.add_one(X)
        y_temp = np.asarray(y)
        X_T = np.transpose(X_temp)
        self.coef_ = np.matmul(np.linalg.inv(np.matmul(X_T, X_temp)), np.matmul(X_T, y_temp))
        if self.fit_intercept==False:
            self.coef_=np.insert(self.coef_,0,0)

    def fit_SVD(self, X, y):
        # Solve the linear regression problem using the SVD of the
        # coefficient matrix
        X_temp, y_temp = self.add_one(X), np.asarray(y)
        u, sigma, vh = np.linalg.svd(X_temp, full_matrices=False)
        sig = np.zeros((len(sigma), len(sigma)))
        for i in range(len(sigma)):
            sig[i][i] = sigma[i]
        self.coef_ = np.matmul(np.transpose(vh),np.matmul(np.linalg.inv(sig), np.matmul(np.transpose(u), y_temp)),)
        if not self.fit_intercept:
            self.coef_=np.insert(self.coef_,0,0)

    def mse_loss(self,coeffecients,X,y):
        # Compute the MSE loss with the learned model
        X_=jax.numpy.asarray(X)
        theta=jax.numpy.asarray(coeffecients)
        y_hat=jax.numpy.matmul(X_,theta)
        y_=jax.numpy.asarray(y)
        return jax.numpy.linalg.norm(x=jax.numpy.subtract(y_hat,y_),ord=2)**2/y.shape[0]
      
    def loss_function(self,coeffecients,X,y,penalty,mu):
      assert mu>=0
      error=self.mse_loss(coeffecients,X,y)
      if penalty=="unregularized":
        error=error
      elif penalty=="l2":
        theta=jax.numpy.linalg.norm(x=jax.numpy.asarray(coeffecients),ord=2)**2/y.shape[0]
        error+=mu*theta
      else:
        theta=jax.numpy.linalg.norm(x=jax.numpy.asarray(coeffecients),ord=1)/y.shape[0]
        error+=mu*theta
      return error

    def compute_gradient(self,X,y,penalty):
        # Compute the analytical gradient (in vectorized form) of the
        # 1. unregularized mse_loss,  and
        # 2. mse_loss with ridge regularization
        # penalty :  specifies the regularization used  , 'l2' or unregularized
        return (2*penalty*self.coef_-2*np.matmul(np.transpose(X),y-np.matmul(X,self.coef_)))/y.shape[0]

    def compute_jax_gradient(self,penalty,X,y,mu):
        # Compute the gradient of the
        # 1. unregularized mse_loss,
        # 2. mse_loss with LASSO regularization and
        # 3. mse_loss with ridge regularization, using JAX
        # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized
        assert mu>=0
        grad_loss=jax.grad(lambda coeffecients:self.loss_function(coeffecients,X,y,penalty,mu))
        return np.asarray(grad_loss(jax.numpy.array(self.coef_)))
        
        

    def fit_gradient_descent(self,X,y, batch_size=1, gradient_type="manual", penalty_type="unregularized",mu=0, num_iters=10, lr=0.01):
        # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
        # batch_size : Number of training points in each batch
        # num_iters : Number of iterations of gradient descent
        # lr : Default learning rate
        # gradient_type : manual or JAX gradients
        # penalty_type : 'l1', 'l2' or unregularized
        X_temp,y_temp=self.add_one(X),np.asarray(y)
        self.coef_=np.random.standard_normal(size=X_temp.shape[1])
        for i in range(num_iters):
          self.all_coef=pd.concat([self.all_coef,pd.Series(self.coef_)],axis=1)
          indices=np.random.randint(low=0,high=X.shape[0],size=batch_size)
          X_sub,y_sub=np.take(X_temp,indices,0),np.take(y_temp,indices,0)
          if gradient_type=="manual":
            assert penalty_type!="l1"
            if penalty_type=="l2":
              self.coef_-=lr*self.compute_gradient(X_sub,y_sub,mu)
            else:
              self.coef_-=lr*self.compute_gradient(X_sub,y_sub,0)
          else:
            self.coef_-=lr*self.compute_jax_gradient(penalty_type,X_sub,y_sub,mu)

    
        
        self.all_coef=pd.DataFrame(np.transpose(np.asarray(self.all_coef)))


    def fit_grad(self, X, y,batch_size, n_iter=100, lr=0.01, lr_type='constant'):
      
        self.all_coef=pd.DataFrame([])
        X_copy=X.copy()
        if (self.fit_intercept==True):
            X_copy.insert(0,-1,1)
            X_copy.columns = np.arange(X_copy.shape[1])
        np.random.seed(42)
        self.coef_=np.random.normal(0, 1, size=X_copy.shape[1])
        thetas=self.coef_

        index=0
        self.all_coef[0]=thetas
        for i in range (n_iter):
            if (index>=X_copy.shape[0]):
                index=0

            if (lr_type=='inverse'):
                lr_1=lr/(i+1)
            else:
                lr_1=lr


            X_1=X_copy.iloc[index:index+batch_size,:]
            y_1=y[index:index+batch_size]
            y_hat=X_1.dot(thetas)

            thetas = thetas - (lr_1*2*((y_1-y_hat).T)@(-X_1.iloc[:])).T     #vectorized updation of thetas
            
            index = index + batch_size 
            self.all_coef[i+1]=thetas

        self.coef_ = thetas

        pass


    def fit_SGD_with_momentum(self,X,y,mu, penalty="l2", beta=0.9, gradient_type="manual",max_iter=3000,batch_size=1,lr=0.01):
        # Solve the linear regression problem using sklearn's implementation of SGD
        # penalty: refers to the type of regularization used (ridge)
        X_temp,y_temp=self.add_one(X),np.asarray(y)
        self.coef_=np.random.standard_normal(size=X_temp.shape[1])
        v=np.zeros(self.coef_.shape[0])
        for i in range(max_iter):
          self.all_coef=pd.concat([self.all_coef,pd.Series(self.coef_)],axis=1)
          indices=np.random.randint(low=0,high=X.shape[0],size=batch_size)
          X_sub,y_sub=np.take(X_temp,indices,0),np.take(y_temp,indices,0)
          grad=None
          if gradient_type=="manual":
            assert penalty!='l1'
            if penalty=="l2":
              grad=self.compute_gradient(X_sub,y_sub,mu)
            else:
              grad=self.compute_gradient(X_sub,y_sub,0)
          else:
            grad=self.compute_jax_gradient(penalty,X_sub,y_sub,mu)
          v=beta*v+grad
          self.coef_=self.coef_-lr*v
        self.all_coef=pd.DataFrame(np.transpose(np.asarray(self.all_coef)))

    def predict(self, X):
        # Funtion to run the LinearRegression on a test data point
        X_temp = np.asarray(X)
        return np.dot(X_temp, self.coef_[1:]) + self.coef_[0] * np.ones(X.shape[0])

    def plot_surface(self, X, y, t_0, t_1):
     
        filenames = []
        fig=plt.figure()
        c = np.arange(-0.5, 6, 0.05)
        m = np.arange(-0.5,6,0.05)
        c,m = np.meshgrid(c,m)
        #axes = fig.gca(projection ='3d')
        axes = fig.add_subplot(projection='3d')
        errs=[]
        for C in range(c.shape[0]):
            e = []
            for M in range(m.shape[1]):
                e.append(np.sum(np.square(y - (c[C][M] + X*m[C][M]))))
            errs.append(e)
        errs=np.array(errs)
        plot=axes.plot_surface(c,m,errs, cmap = cm.coolwarm)
        axes.view_init(20, 120)
        axes.set_xlabel(r'$\theta_0$')
        axes.set_ylabel(r'$\theta_1$')
        fig.colorbar(plot, shrink=0.8)
        for iter,(t0,t1) in enumerate(zip(t_0,t_1)):
            
            y_hat=t0 + X*t1
            err=np.sum(np.square(y-y_hat))
            axes.set_title(f'Error = {err:.2f}')
            print(iter,t0,t1,err)
            
            p = Circle((t0, t1), 0.05,color='black')
            axes.add_patch(p)
            art3d.pathpatch_2d_to_3d(p, z=err, zdir="z")
            filename = f'{iter}.png'
            filenames.append(filename)
            plt.savefig(filename)

        with imageio.get_writer('surface.gif',mode = 'I',fps=5) as writer:
            for filename in filenames:
                image = imageio.imread(filename)
                writer.append_data(image)

            
        for filename in set(filenames):
            os.remove(filename)

        pass

    def plot_line_fit(self, X, y, t_0, t_1):
      

        filenames=[]
        X_np = np.array(X)
        y_np = np.array(y)
        for iter,(t0,t1) in enumerate(zip(t_0,t_1)):
            fig=plt.figure()
            plt.ylim(0,35)
            plt.xlabel("Feature")
            plt.ylabel("Output")
            plt.title(f"y = {t1:0.2f}x + {t0:0.2f}")
            plt.scatter(X_np,y_np)
            
            y_hat=t0 + X*t1
            err=np.sum(np.square(y-y_hat))
            plt.plot(X_np, y_hat)
            filename = f'{iter}.png'
            filenames.append(filename)
            plt.savefig(filename)
            
        with imageio.get_writer('line_fit.gif',mode = 'I',fps=2) as writer:
            for filename in filenames:
                image = imageio.imread(filename)
                writer.append_data(image)

        for filename in set(filenames):
            os.remove(filename)
        pass

    def plot_contour(self, X, y, t_0, t_1):
       
        filenames = []
        fig=plt.figure()
        c = np.arange(-0.5, 6, 0.05)
        m = np.arange(-0.5, 6, 0.05)
        c,m = np.meshgrid(c,m)

        errs=[]
        for C in range(c.shape[0]):
            e = []
            for M in range(m.shape[1]):
                e.append(np.sum(np.square(y - (c[C][M] + X*m[C][M]))))
            errs.append(e)
        errs=np.array(errs)

        plot=plt.contourf(c,m,errs)
        fig.colorbar(plot, shrink=0.8)
        plt.xlabel("c")
        plt.ylabel("m")
        plt.title('Contour plot')

        for iter,(t0,t1) in enumerate(zip(t_0,t_1)):
            
            y_hat=t0 + X*t1
            err=np.sum(np.square(y-y_hat))
            # print(iter,t0,t1,err)
            plt.scatter(t0, t1,s=28, marker ='3', color = 'red')
            filename = f'{iter}.png'
            filenames.append(filename)
            plt.savefig(filename)

        with imageio.get_writer('contour.gif',mode = 'I',fps=2) as writer:
            for filename in filenames:
                image = imageio.imread(filename)
                writer.append_data(image)

        for filename in set(filenames):
            os.remove(filename)
        
        pass

